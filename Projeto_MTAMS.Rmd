---
title: "Projeto Conclusão Mineração de Texto e Análise de Mídias Sociais"
author: "Mayara Yonemura"
date: '2022-08-17'
output: html_document
---

```{r setup, include=FALSE}
setwd("C://Users/Remessa Online/Desktop/May/PersonnalStuff/FGV/Classes/Analise_Midias_Sociais/Projeto_Conclusao/")
knitr::opts_chunk$set(echo = TRUE)
```

```{r importando bibliotecas, echo=FALSE, message=FALSE, warning=FALSE}
library(rvest)
library(stringr)
library(tidyverse) 
library(tm)
library(igraph)
library(wordcloud)
library(urltools) 
library(spacyr)
library(gtools)
library(spacyr)
spacy_download_langmodel("pt_core_news_sm")
```

```{r funcao titulo materia, echo=FALSE}
# Funcao para coletar os links de cada noticia
scrape_post_links <- function(site) {
  cat(paste0(site, "\n"))
  
  source_html <- read_html(site)
  
  links <- source_html %>%
    html_nodes("div.widget--info__text-container") %>%
    html_nodes("a") %>%
    html_attr("href")
  
  links <- links[!is.na(links)]
  
  return(links)
}
```


<br>
```{r execucao leitura links, echo=FALSE}
root <- "https://g1.globo.com/busca/?q=Inteligencia+Artificial"
 
## Extração do link das páginas das materias
all_pages <- c(root, paste0(root, "&page=", 1:20))

## Extração dos links de cada materia
all_links <- lapply(all_pages, scrape_post_links)
all_links <- unlist(all_links)

length(all_links)

all_links
```
<br>


```{r excluir links, echo=FALSE}
cleaned_links <- map_chr(all_links, ~{
  .x %>% 
    urltools::param_get() %>% 
    pull(u) %>% 
    urltools::url_decode()
})


# reter apenas links que falam de tecnologia
cleaned_links <- cleaned_links %>% .[str_detect(.,  "https://g1.globo.com/tecnologia/")]

cleaned_links

# nao reter links do globoplay
##cleaned_links <- cleaned_links %>% .[!str_detect(.,  "globoplay")]
```


<br>
```{r  funcao texto materia, echo=FALSE}
# Funcao para coletar o texto da materia em cada link
scrape_post_body <- function(site) { 
  
  text <- tryCatch({
    cat(paste0(site, "\n"))
    body <- site %>%
      read_html %>%
      html_nodes("article") %>%
      html_nodes("p.content-text__container")  %>%
      html_text %>% 
      paste(collapse = '')
    
  }, error = function(e){
    cat(paste("ERRO 404", "\n"))
    body <- NA
  })
  
  return(body)
}
```

<br>
```{r execucao texto materia, echo=FALSE}
data <- lapply(cleaned_links, scrape_post_body)
data <- lapply(data, 
               function(item) paste(unlist(item),
                                    collapse = ''))

data

```

<br>
```{r limpeza textos, echo=FALSE}
## Limpeza de caracteres e uniformidade do texto
cleaned <- tolower(data)
cleaned <- removeNumbers(cleaned)
cleaned <- removePunctuation(cleaned)
cleaned <- str_trim(cleaned)
cleaned <- removeWords(cleaned, c(stopwords("pt"),
                                  "disse", "diz",
                                  "fazer", "ter", "deve",
                                  "pode", "acho", "posso",
                                  "vai", "sido",
                                  "sendo", "tipo","dizer",
                                  "faz", "ser",
                                  "qualquer", "sobre", 
                                  "meio", "assim","nesta", 
                                  "alguns", "parece", "cada", 
                                  "diferente", "ver", "pouco", "the", 
                                  "outros", "vez", "partir", "disso", 
                                  "usar", "havia", "durante", 
                                  "mostra", "simplesmente", "fez",
                                  "tornar", "muitos", "apesar", "desse", 
                                  "anunciou", "language", "simples", 
                                  "tenta", "dois", "usada", "informou", 
                                  "algumas", "geralmente", "assunto", 
                                  "realmente", "gerar", "entender", "considerou", 
                                  "inteligência", "artificial", "sim")
                       )


## Converter pra Vetor
cleaned_corpus <- Corpus(VectorSource(cleaned))
doc_object <- TermDocumentMatrix(cleaned_corpus)
doc_matrix <- as.matrix(doc_object)

## Contagem de palavras
counts <- sort(rowSums(doc_matrix),decreasing=TRUE)

## Limpeza de caracteres que não são letras
counts <- counts[grepl("^[a-z]+$", names(counts))]

## Cria o Dataset com todas as palavras e a frequência
frame_counts <- data.frame(word = names(counts), freq = counts)

```

<br>
```{r wordcloud, echo=T, warning = F, message = F, fig.height = 5, fig.width = 10, fig.align = 'center'}
wordcloud(words = frame_counts$word,
          freq = frame_counts$freq,
          min.freq = 4,
          max.words=200, random.order=FALSE,
          rot.per=0, 
          colors=brewer.pal(8, "Dark2"))

```



<br>
Como estamos limitando a 200 palavras na nuvem, a limpeza das palavras de ligação em portugués fazem a diferença para que novas palavras surjam e algumas fiquem cada vez mais evidente.
Assim como, algumas palavras fora do contextos, nos causa curiosidade sobre o título da matéria.




<br>
```{r adjacencias, echo=FASEL}
spacy_initialize(model = "pt_core_news_sm")
entities <- spacy_extract_entity(unlist(data))
head(entities)
```


```{r convert_to_edges, echo=T, message=T, warning=T}
# group entities by document
filtered_entities <- subset(entities, entities["ent_type"] == "ORG" | 
                                      entities["ent_type"] == "PER" )
edges <- filtered_entities %>% 
         group_by(doc_id) %>%   
         summarise(entities = paste(text, collapse = ","))
# remove duplicated for the same document
edges <- lapply(str_split(edges$entities, ","), 
                function(t){unique(unlist(t))})
# Auxiliary functions for creating adjancnt
get_adjacent_list <- function(edge_list) {
    if (length(edge_list) > 2)
      adjacent_matrix <- combinations(length(edge_list),
                                      2, edge_list)
}
adjacent_matrix <- edges %>% 
                   lapply(get_adjacent_list) %>%
                   reduce(rbind)
```

<br>
```{r criando o grafo, echo=FALSE}
df <- as_tibble(adjacent_matrix, colnames=c('source', 'target'))
weighted_edgelist <- df %>%
                     group_by(V1, V2) %>% 
                     summarise(weight=n())
news_graph <- weighted_edgelist %>%  graph_from_data_frame(directed=F)

write_graph(news_graph, 
            file = 'C://Users/Remessa Online/Desktop/May/PersonnalStuff/FGV/Classes/Analise_Midias_Sociais/Projeto_Conclusao/newsgraph.graphml', 
            format = 'graphml')
```















